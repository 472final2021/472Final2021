# -*- coding: utf-8 -*-
"""q_learning_trading.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NISNRcNfUt6QeTyM7L_uV4MTUyuA0Awl
"""

# !pip install tensorflow-gpu==1.15.0 tensorflow==1.15.0 stable-baselines gym-anytrading gym yfinance finta

"""Imports"""

#yahoo finance
import yfinance as yf

#gym
import gym
import gym_anytrading
from gym_anytrading.envs import StocksEnv

#pre-processing
import numpy as np
import random
import pandas as pd
from pandas_datareader import data as pdr
from matplotlib import pyplot as plt
from sklearn import preprocessing

#torch
import torch.nn as nn 
import torch.nn.functional as F
import torch.optim as optim
import torch as T

#finta
from finta import TA

"""Extacting data from sentiment score files, yahoo finance library and finta library"""

def extract_data(filename, start, end):
  yf.pdr_override()

  reddit_scores_df = pd.read_csv(filename) 
  reddit_scores_df['Date'] =  pd.to_datetime(reddit_scores_df['Date'], infer_datetime_format=True)
  reddit_scores_df = reddit_scores_df.set_index('Date')

  datasets = []
  tickers = [col for col in reddit_scores_df.columns]
  for t in tickers:
    if t == 'SP500':
      t = 'SPY'
    stock_df = pdr.get_data_yahoo(t, start=start, end=end)
    stock_df['RSI'] = TA.RSI(stock_df)
    stock_df = stock_df.merge(reddit_scores_df[t if t != 'SPY' else 'SP500'], on='Date')
    stock_df.columns = [*stock_df.columns[:-1], 'Sentiment']
    stock_df.fillna(0, inplace=True)
    datasets.append(stock_df)
    
  return datasets, tickers

"""Custom stock trading environment"""

# reference https://github.com/AminHP/gym-anytrading
class RedditStockEnv(StocksEnv):
    _process_data = add_signals

"""Utility methods (normalize, add signals, create environments)"""

def normalize_dataframe(df):
  x = df.values
  min_max_scaler = preprocessing.MinMaxScaler()
  x_scaled = min_max_scaler.fit_transform(x)
  df_norm = pd.DataFrame(x_scaled)

  for (i, column_name) in enumerate(df.columns):
    df[df.columns[i]] = (df_norm[i]).values

  return df

# reference https://github.com/AminHP/gym-anytrading
def add_signals(env):
    start = env.frame_bound[0] - env.window_size
    end = env.frame_bound[1]
    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]
    signal_features = env.df.loc[:, ['Adj Close', 'Low', 'Volume', 'RSI', 'Sentiment']].to_numpy()[start:end]
    return prices, signal_features

def generate_environments(df, train_start, train_end, test_start, test_end, window_size):
  training_envs = []
  testing_envs = []

  for df in datasets:
    norm_df = normalize_dataframe(df)
    training_env = RedditStockEnv(df=norm_df, window_size=window_size, frame_bound=(train_start, train_end))
    testing_env = RedditStockEnv(df=norm_df, window_size=window_size, frame_bound=(test_start, test_end))
    training_envs.append(training_env)
    testing_envs.append(testing_env)

  return training_envs, testing_envs

"""Code for Linear Deep Q Network"""

class LinearDeepQNetwork(nn.Module):
	def __init__(self, lr, n_actions, input_dims):
		super(LinearDeepQNetwork, self).__init__()
		self.fc1 = nn.Linear(*input_dims, 64) 
		self.fc2 = nn.Linear(64, 32) 
		self.fc3 = nn.Linear(32, 16) 
		self.fc4 = nn.Linear(16, n_actions)
		self.optimizer = optim.Adam(self.parameters(), lr=lr)
		self.loss = nn.MSELoss()
		self.elu = nn.ELU()
		self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
		self.to(self.device)

	def forward(self, data):
		layer1 = self.elu(self.fc1(data))
		layer2 = self.elu(self.fc2(layer1))
		layer3 = self.elu(self.fc3(layer2))
		layer4 = self.fc4(layer3)
		return layer4

"""Code for Q Learning Agent

Adapted and referenced from:

https://www.udemy.com/course/deep-q-learning-from-paper-to-code by Phil Tabor
"""

class NNAgent:
    def __init__(self, n_actions, input_dims, lr, gamma, epsilon, epsilon_min, epsilon_dec):
        self.lr = lr
        self.gamma = gamma
        self.n_actions = n_actions
        self.action_space = [i for i in range(self.n_actions)]
        self.input_dims = input_dims
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_dec = epsilon_dec

        self.losses = []

        self.nn = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)

    def choose_action(self, state):
        if np.random.random() < self.epsilon:
            return np.random.choice(self.action_space)
        else:
            state = T.tensor(state, dtype=T.float).to(self.nn.device)
            actions = self.nn.forward(state)
            return T.argmax(actions).item()

    def predict(self, state):
      state = T.tensor(state, dtype=T.float).to(self.nn.device)
      actions = self.nn.forward(state)
      return T.argmax(actions).item()

    def decrement_epsilon(self):
        self.epsilon = self.epsilon - self.epsilon_dec if self.epsilon > self.epsilon_min else self.epsilon_min

    def learn(self, state, action, reward, state_prime):
        self.nn.optimizer.zero_grad()

        states = T.tensor(state, dtype=T.float).to(self.nn.device)
        actions = T.tensor(action).to(self.nn.device)
        reward = T.tensor(reward).to(self.nn.device)
        states_prime = T.tensor(state_prime, dtype=T.float).to(self.nn.device)

        q_pred = self.nn.forward(states)[actions]
        q_next = self.nn.forward(states_prime).max()

        q_targ = reward + self.gamma * q_next
        loss = self.nn.loss(q_targ.float(), q_pred.float()).to(self.nn.device)

        self.losses.append(loss.item())
        loss.backward()
      
        self.nn.optimizer.step()
        self.decrement_epsilon()

"""TRAINING"""

def train_environment(env, episodes, lr, gamma, eps, eps_min, eps_dec):
  train_env = env
  X, Y = train_env.observation_space.shape
  agent = NNAgent(n_actions=train_env.action_space.n, input_dims=(X * Y,),
                    lr=lr, gamma=gamma, epsilon=eps, epsilon_min=eps_min, 
                    epsilon_dec=eps_dec)
  
  for i in range(episodes):
    obs = train_env.reset()
    obs = obs.flatten()
    done = False
    while not done: 
        action = agent.choose_action(obs)
        obs_, reward, done, info = train_env.step(action)
        obs_ = obs_.flatten()
        agent.learn(obs, action, reward, obs_)
        obs = obs_

  return agent

"""TESTING"""

def test_environment(env, agents):
  tests = []
  testing_env = env
  obs = testing_env.reset()
  done = False
  while not done:
    action = agent.predict(obs.flatten())
    obs, reward, done, info = testing_env.step(action)
  return testing_env

"""PLOTTING"""

def plot_environment(env):
  plt.figure(figsize=(15,6))
  plt.cla()
  env.render_all()
  plt.show()

"""Training and testing the model"""

data_frames, tickers = extract_data('reddit_sentiment_scores.csv', start='2019-01-09', end='2020-12-31')
train_envs, test_envs = generate_environments(data_frames, train_start=5, train_end=400, test_start=405, test_end=460, window_size=5)

train_env = train_envs[5]
agent = train_environment(train_env, 150, lr=1e-4, gamma=.90, eps=1.0, eps_min=0.15, eps_dec=2e-5)
tested_env = test_environment(test_envs[5], agent)

plot_environment(tested_env)