# -*- coding: utf-8 -*-
"""Reddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17aM8IhRO4YWTd0ZKwMh-uG0rW9eq3vjZ

Scraping posts from Reddit threads


1. r/WallStreetBets
2. r/Stocks
3. r/Investing

Reference:
- (ticker code)
https://medium.com/@tom.santinelli/scraping-reddits-wall-street-bets-for-popular-stock-tickers-38ed5202affc
- (stock tickers)
https://www.nasdaq.com/market-activity/stocks/screener
- (Flair NLP)
https://github.com/flairNLP/flair
- (NLP code)
https://towardsdatascience.com/sentiment-analysis-for-stock-price-prediction-in-python-bed40c65d178
- https://github.com/dmarx/psaw

Dependencies and initialization
=========
"""

!pip install praw
!pip install psaw
!pip install flair
import flair
import praw
import pandas as pd
import re  
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
import datetime as dt
from psaw import PushshiftAPI

"""Declare a reddit instance (used a dummy account)"""

reddit = praw.Reddit(client_id='6pKo4YmL2zkS0Q', \
                     client_secret='kCZTMPNF7Do43YNPbWlWihtbQFG0cg', \
                     user_agent='scraper', \
                     )

"""Declare subredit instances: wallstreetbets, stocks, investing"""

wsb_subr = reddit.subreddit('wallstreetbets')
stocks_subr = reddit.subreddit('stocks')
invest_subr = reddit.subreddit('investing')

"""Generating the most commonly mentioned stock tickers
========

1. Grabbing the title and content of each post and appending to a dataframe
"""

df = []

for post in wsb_subr.hot(limit = 500):
    content = {
        "title": post.title,
        "text": post.selftext
    }
    df.append(content)

for post in stocks_subr.hot(limit = 500):
    content = {
        "title": post.title,
        "text": post.selftext
    }
    df.append(content)

for post in invest_subr.hot(limit = 500):
    content = {
        "title": post.title,
        "text": post.selftext
    }
    df.append(content)

df = pd.DataFrame(df)

"""2. Regular expression for filtering """

regex = re.compile('[^a-zA-Z ]')

"""3. Analyze word frequency"""

word_dict = {}

for (index, row) in df.iterrows():
    
    # title
    title = row['title']
    title = regex.sub('',title)
    title_words = title.split(' ')

    # content 
    content = row['text']
    content = regex.sub('',content)
    content_words = content.split(' ')

    # combine
    words = title_words + content_words
    for x in words:
        if x in ['A', 'B', 'GO', 'ARE', 'ON']:
            pass
        elif x in word_dict:
            word_dict[x] += 1
        else:
            word_dict[x] = 1


word_df = pd.DataFrame.from_dict(list(word_dict.items())).rename(columns = {0:"Term",1:"Frequency"})

"""4. Read the csv of stock tickes into a dataframe"""

ticker_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CMSC472_2021_Final_Project/tickers.csv').rename(columns = {'Symbol':'Term', 'Name':'Company_Name'})

"""5. Inner join of the two datasets, and then sort by the frequency they are mentioned from. """

stonks_df = pd.merge(ticker_df, word_df, on="Term")
stonks_df = stonks_df.sort_values(by = "Frequency",ascending=False)

idx = 0;
chosen_comp = [];
for t in stonks_df['Term']:
    chosen_comp.append(t)

chosen_comp = chosen_comp[0:3]
chosen_comp

"""Now stonks_df is a list of stock tickers and their frequency in 500 hot wall street bets, stocks, and investing posts.

Retrieving the posts that mentioned the top 10 stock tickers
==========

1. Adding a column "Content" into the stonks_df dataframe
"""

if stonks_df.__contains__("Content") == False:
    stonks_df.insert(len(stonks_df.columns), "Content", " ")

"""2. For the top 10 mentioned stocks, retrieve all the comments and titles that mentioned it and store in the corresponding "Content" section"""

regex = re.compile('[^a-zA-Z. ]')
whitespace = re.compile(r"\s+")
web_address = re.compile(r"(?i)http(s):\/\/[a-z0-9.~_\-\/]+")

for i in range(10):
    t = stonks_df['Term'][i]


    for (index, row) in df.iterrows():
        
        # title
        title = row['title']
        title = regex.sub(' ',title)
        title = whitespace.sub(' ', title)
        title_sentence = title.split('.')
        title_words = title.split(' ')

        # content 
        content = row['text']
        content = web_address.sub(' ', content)
        content = regex.sub(' ',content)
        content = whitespace.sub(' ', content)
        content_sentence = content.split('.')
        content_words = content.split(' ')

        # combine
        sentence = title_sentence + content_sentence
        
        words = title_words + content_words
        for x in words:
            if x == t and sentence != " " and sentence != "":
                stonks_df.at[i,'Content'] = sentence
                break

"""Now, we are ready to run NLP with each of the contents associated with top mentioned stocks

NLP with the comments
==========

1. Install Flair for NLP
"""

!pip install flair

"""2. Initialize flair"""

import flair
sentiment_model = flair.models.TextClassifier.load('en-sentiment')

"""3. Add another column "Sentiment" into the dataframe """

if stonks_df.__contains__("Sentiment") == False:
    stonks_df.insert(len(stonks_df.columns), "Sentiment", 0.00)

"""4. Generating sentiment score for the top stocks and store in the data fram"""

for i in range(10):
    neg = 0
    pos = 0
    slen = 0
    for s in stonks_df["Content"][i]:
        s = s.strip()
        if s == "":
            slen -= 1
            continue

        s = flair.data.Sentence(s)
        sentiment_model.predict(s)
        
        score = s.labels[0].score
        value = s.labels[0].value
        if value == 'POSITIVE':
            pos += score
        else:  
            neg += score
        slen +=1
    total_score = (pos - neg) / slen
    stonks_df.at[i,'Sentiment'] = total_score



"""Combine the above into methods
==========
"""

import flair
sentiment_model = flair.models.TextClassifier.load('en-sentiment')

"""** The method below outputs the top mentioned stock tickers from r/wallstreet, r/stocks, r/investing ** """

# Input
# - ticker_df (a data frame that contains the ticker lists)
# - limit (the number of hot posts from each subreddit)
# output 
# - df (data frame that contains the number of times mentioned for each ticker)

def topMentionedReddit(ticker_df, limit):
    reddit = praw.Reddit(client_id='6pKo4YmL2zkS0Q', \
                     client_secret='kCZTMPNF7Do43YNPbWlWihtbQFG0cg', \
                     user_agent='scraper', \
                     )
    wsb_subr = reddit.subreddit('wallstreetbets')
    stocks_subr = reddit.subreddit('stocks')
    invest_subr = reddit.subreddit('investing')

    regex = re.compile('[^a-zA-Z. ]')
    whitespace = re.compile(r"\s+")
    web_address = re.compile(r"(?i)http(s):\/\/[a-z0-9.~_\-\/]+")

    df = []
    for post in wsb_subr.hot(limit = limit):
        content = {
            "title": post.title,
            "text": post.selftext
        }
        df.append(content)

    for post in stocks_subr.hot(limit = limit):
        content = {
            "title": post.title,
            "text": post.selftext
        }
        df.append(content)

    for post in invest_subr.hot(limit = limit):
        content = {
            "title": post.title,
            "text": post.selftext
        }
        df.append(content)

    df = pd.DataFrame(df)

    regex = re.compile('[^a-zA-Z ]')
    word_dict = {}

    for (index, row) in df.iterrows():
        
        # title
        title = row['title']
        title = regex.sub('',title)
        title_words = title.split(' ')

        # content 
        content = row['text']
        content = regex.sub('',content)
        content_words = content.split(' ')

        # combine
        words = title_words + content_words
        for x in words:
            if x in ['A', 'B', 'GO', 'ARE', 'ON']:
                pass
            elif x in word_dict:
                word_dict[x] += 1
            else:
                word_dict[x] = 1


    word_df = pd.DataFrame.from_dict(list(word_dict.items())).rename(columns = {0:"Term",1:"Frequency"})
    #ticker_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CMSC472_2021_Final_Project/tickers.csv').rename(columns = {'Symbol':'Term', 'Name':'Company_Name'})

    stonks_df = pd.merge(ticker_df, word_df, on="Term")
    stonks_df = stonks_df.sort_values(by = "Frequency",ascending=False)

    return stonks_df

"""** The method below looks up a specific stock ticker within the given time frame, scrap the posts that mention the specific stock on the day and analyze the sentiment score for the day **"""

# Input: 
# - ticker (string)
# - begin_date (datetime object)
# - end_date(datetime object)
# - verbose (Boolean) True for for printing results; false otherwise
# output:
# - cvs file that contains the date and the sentiment score from the given time frame

def redditDailyScore(ticker, begin_date, end_date, verbose):
    api = PushshiftAPI()
    day_count = (end_date - begin_date).days

    regex = re.compile('[^a-zA-Z. ]')
    whitespace = re.compile(r"\s+")
    web_address = re.compile(r"(?i)http(s):\/\/[a-z0-9.~_\-\/]+")

    dateDF = pd.DataFrame();
    dateDF.insert(0,'Date',0)
    dateDF.insert(1, "Sentiment",0)
    dateIdx = 0;

    for single_date in (begin_date + dt.timedelta(n) for n in range(day_count)):
        
        try:
            after =int(single_date.timestamp())
            before = int((single_date +dt.timedelta(1)).timestamp())
            gen = api.search_submissions(before = before,
                                after = after,
                                q = ticker,        
                                filter = ['id','author','title','num_comments','selftext'],
                                subreddit='wallstreetbets',
                                limit=1000)
            
            gen2 = api.search_submissions(before = before,
                                after = after,
                                q = ticker,        
                                filter = ['id','author','title','num_comments','selftext'],
                                subreddit='stocks',
                                limit=1000)
            
            gen3 = api.search_submissions(before = before,
                                after = after,
                                q = ticker,        
                                filter = ['id','author','title','num_comments','selftext'],
                                subreddit='investing',
                                limit=1000)
            
            df = pd.DataFrame(gen)
            df2 = pd.DataFrame(gen2)
            df3 = pd.DataFrame(gen3)

            content = []
            

            for (index, row) in df.iterrows():
            
                # title
                title = row['title']
                title = regex.sub(' ',title)
                title = whitespace.sub(' ', title)
                title_sentence = title.split('.')

                # body
                body = row['selftext']
                body = web_address.sub(' ', body)
                body = regex.sub(' ',body)
                body = whitespace.sub(' ', body)
                body_sentence = body.split('.')

                # combine
                sentence = title_sentence + body_sentence
                content = np.append(sentence,content)
            
            for (index, row) in df2.iterrows():
            
                # title
                title = row['title']
                title = regex.sub(' ',title)
                title = whitespace.sub(' ', title)
                title_sentence = title.split('.')

                # body
                body = row['selftext']
                body = web_address.sub(' ', body)
                body = regex.sub(' ',body)
                body = whitespace.sub(' ', body)
                body_sentence = body.split('.')

                # combine
                sentence = title_sentence + body_sentence
                content = np.append(sentence,content)
            
            for (index, row) in df3.iterrows():
            
                # title
                title = row['title']
                title = regex.sub(' ',title)
                title = whitespace.sub(' ', title)
                title_sentence = title.split('.')

                # body
                body = row['selftext']
                body = web_address.sub(' ', body)
                body = regex.sub(' ',body)
                body = whitespace.sub(' ', body)
                body_sentence = body.split('.')

                # combine
                sentence = title_sentence + body_sentence
                content = np.append(sentence,content)
          

            neg = 0
            pos = 0
            slen = 0
            for s in content:
                if s == "":
                    continue

                if s == " ":
                    continue

                s = flair.data.Sentence(s)
                sentiment_model.predict(s)
                
                score = s.labels[0].score
                value = s.labels[0].value
                if value == 'POSITIVE':
                    pos += score
                else:  
                    neg += score
                slen +=1

            if slen == 0:
                total_score = 0
            else: 
                total_score = (pos - neg) / slen
            
          

            dateDF.at[dateIdx,'Date'] = single_date
            dateDF.at[dateIdx, 'Sentiment'] = total_score
            dateIdx += 1

            if verbose:
                s = str(single_date) + ' ' + str(total_score)
                print(s) 
        
        except:
            dateDF.at[dateIdx,'Date'] = single_date
            dateDF.at[dateIdx, 'Sentiment'] = 0
            dateIdx += 1
            if verbose:
                s = str(single_date) + ' ' + "Non-String"
                print(s)
            
            continue

    return dateDF

"""Use topMentionReddit to generate the top 10 tickers
==========


"""

ticker_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CMSC472_2021_Final_Project/tickers.csv').rename(columns = {'Symbol':'Term', 'Name':'Company_Name'})
stonks_df = topMentionedReddit(ticker_df,500)

company = stonks_df['Term']
idx = 0;
chosen_comp = [];
for t in stonks_df['Term']:
    chosen_comp.append(t)

chosen_comp = chosen_comp[0:10]
chosen_comp

"""Use redditDailyScore to generate csv files that contains the sentiment score from 2019 to 2021
==========
"""

resultDF = redditDailyScore('M', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF.to_csv("M_2019.csv") #done

resultDF2 = redditDailyScore('DD', dt.datetime(2019,1,1,0,0), dt.datetime(2019,10,1,0,0),True)
resultDF2.to_csv("DD_2019_1.csv") #done

resultDF2_2 = redditDailyScore('DD', dt.datetime(2019,10,2,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF2_2.to_csv("DD_2019_2.csv") #done

resultDF3 = redditDailyScore("K",dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF3.to_csv("K_2019.csv")#done

resultDF_SP = redditDailyScore('SP', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_SP.to_csv("SP_2019.csv")#done

resultDF_SP = redditDailyScore('UWMC', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_SP.to_csv("UWMC_2019.csv")#done

resultDF_SP1 = redditDailyScore('FOR', dt.datetime(2019,1,1,0,0), dt.datetime(2019,5,6,0,0),True)
resultDF_SP1.to_csv("FOR_2019_1 .csv")#done

resultDF_SP1.to_csv("FOR_2019_1 .csv")#done

resultDF_FOR2 = redditDailyScore('FOR', dt.datetime(2019,5,8,0,0), dt.datetime(2019,7,2,0,0),True)
resultDF_FOR2.to_csv("FOR_2019_2 .csv")#done

resultDF_FOR3 = redditDailyScore('FOR', dt.datetime(2019,7,2,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_FOR3.to_csv("FOR_2019_3 .csv")#done

resultDF_G = redditDailyScore('G', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_G.to_csv("G_2019.csv")#done

resultDF_PLTR = redditDailyScore('PLTR', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_PLTR.to_csv("PLTR_2019.csv")#done

resultDF_TSLA = redditDailyScore('TSLA', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_TSLA.to_csv("TSLA_2019.csv")#done

resultDF_SP500= redditDailyScore('SP500', dt.datetime(2019,1,1,0,0), dt.datetime(2020,1,1,0,0),True)
resultDF_SP500.to_csv("SP500_2019.csv")#done

resultDF_M_1 = redditDailyScore('M', dt.datetime(2020,1,1,0,0), dt.datetime(2020,3,16,0,0),True)
resultDF_M_1.to_csv("M_2020_1.csv") #done #3/16 not array like object?

resultDF_M_2 = redditDailyScore('M', dt.datetime(2020,3,17,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_M_2.to_csv("M_2020_2.csv") #done #3/16 not array like object?

resultDF_DD = redditDailyScore('DD', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_DD.to_csv("DD_2020.csv") #done

resultDF_SP = redditDailyScore('SP', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_SP.to_csv("SP_2020.csv") #done

resultDF_K = redditDailyScore('K', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_K.to_csv("K_2020.csv") #done

resultDF_FOR = redditDailyScore('FOR', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_FOR.to_csv("FOR_2020.csv") #done

resultDF_G = redditDailyScore('G', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_G.to_csv("G_2020.csv") #done

resultDF_TSLA = redditDailyScore('TSLA', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_TSLA.to_csv("TSLA_2020.csv") #done

resultDF_SP500= redditDailyScore('SP500', dt.datetime(2020,1,1,0,0), dt.datetime(2021,1,1,0,0),True)
resultDF_SP500.to_csv("SP500_2019.csv")#done